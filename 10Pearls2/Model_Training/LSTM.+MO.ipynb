{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6cff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Multi-horizon AQI Forecasting with LSTM (mirrors your LGBM pipeline) ===\n",
    "from pathlib import Path\n",
    "import json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- Imports -----------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\"])\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a334e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "PROJECT_ROOT  = Path.cwd().parent        # notebook inside Model_training/\n",
    "DATA_PATH     = PROJECT_ROOT / \"preprocessed_aqi_data (3).csv\"\n",
    "FEATURES_JSON = PROJECT_ROOT / \"final_feature_list.json\"\n",
    "\n",
    "OUT_DIR       = PROJECT_ROOT / \"predictions\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_FILE   = OUT_DIR / \"lstm_predicted_aqi_72hrs.csv\"\n",
    "\n",
    "WINDOW_SIZE     = 24       # past hours per sample\n",
    "PREDICT_HORIZON = 72       # next 72 hours (3 days)\n",
    "TARGET_COL      = \"us_aqi\"\n",
    "TIME_COL_CANDS  = [\"time\", \"datetime\"]\n",
    "\n",
    "# Training\n",
    "EPOCHS          = 40\n",
    "BATCH_SIZE      = 256\n",
    "LR              = 1e-3\n",
    "WD              = 1e-4\n",
    "PATIENCE        = 8\n",
    "HIDDEN_SIZE     = 128\n",
    "NUM_LAYERS      = 2\n",
    "DROPOUT         = 0.2\n",
    "RANDOM_SEED     = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "646b34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Reproducibility -----------------\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ------------------- LOAD & PARSE TIME (DAY-FIRST) -------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# pick time column name automatically\n",
    "for c in TIME_COL_CANDS:\n",
    "    if c in df.columns:\n",
    "        TIME_COL = c\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(f\"No datetime column found. Expected one of: {TIME_COL_CANDS}\")\n",
    "\n",
    "# clean & parse as day-first to match your “4/8/25 = 4 Aug 2025”\n",
    "raw_time = (\n",
    "    df[TIME_COL].astype(str)\n",
    "      .str.strip()\n",
    "      .str.replace(\"\\u00A0\", \" \", regex=False)\n",
    "      .str.replace(\"\\u202F\", \" \", regex=False)\n",
    ")\n",
    "df[TIME_COL] = pd.to_datetime(raw_time, dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# sort chronologically\n",
    "df = df.sort_values(TIME_COL).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4acbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- FEATURES -------------------\n",
    "feat_cols = json.loads(FEATURES_JSON.read_text())\n",
    "missing = [c for c in feat_cols + [TARGET_COL, TIME_COL] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in data: {missing}\")\n",
    "\n",
    "# ------------------- Helper: build windows -------------------\n",
    "def build_sequences_limit(frame, features, target, window, horizon):\n",
    "    \"\"\"Just to figure out how many usable windows exist (like your LGBM first pass).\"\"\"\n",
    "    n = len(frame)\n",
    "    limit = n - window - horizon\n",
    "    return limit\n",
    "\n",
    "def build_sequences_seq(frame, features, target, window, horizon):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X: (N, T, F) with oldest->newest order (T==window)\n",
    "      Y: (N, H)\n",
    "    \"\"\"\n",
    "    Xs, Ys = [], []\n",
    "    Xmat = frame[features].values\n",
    "    yvec = frame[target].values\n",
    "    n = len(frame)\n",
    "    limit = n - window - horizon\n",
    "    for i in range(limit):\n",
    "        Xs.append(Xmat[i:i+window])                        # (T, F)\n",
    "        Ys.append(yvec[i+window:i+window+horizon])         # (H,)\n",
    "    return np.asarray(Xs), np.asarray(Ys), limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57873d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- TRAIN-ONLY WINSORIZATION + IMPUTE + SCALE -------------------\n",
    "# First pass to know how many windows we’ll have\n",
    "limit = build_sequences_limit(df, feat_cols, TARGET_COL, WINDOW_SIZE, PREDICT_HORIZON)\n",
    "if limit <= 0:\n",
    "    raise ValueError(\"Not enough rows to make sliding windows. Add more data.\")\n",
    "\n",
    "# Map “train windows” back to raw rows used by their inputs (same logic you used)\n",
    "train_windows = int(limit * 0.8)\n",
    "raw_end_for_train_inputs = (train_windows - 1) + WINDOW_SIZE\n",
    "raw_end_for_train_inputs = max(raw_end_for_train_inputs, WINDOW_SIZE)\n",
    "\n",
    "# Winsorize on TRAIN INPUT rows only\n",
    "numeric_feats = [c for c in feat_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "low = df.loc[:raw_end_for_train_inputs, numeric_feats].quantile(0.01).to_dict()\n",
    "high = df.loc[:raw_end_for_train_inputs, numeric_feats].quantile(0.99).to_dict()\n",
    "for c in numeric_feats:\n",
    "    df[c] = df[c].clip(lower=low[c], upper=high[c])\n",
    "\n",
    "# Median impute using TRAIN INPUT rows only, then apply to all\n",
    "train_medians = df.loc[:raw_end_for_train_inputs, feat_cols].median(numeric_only=True).to_dict()\n",
    "df[feat_cols] = df[feat_cols].fillna(train_medians)\n",
    "\n",
    "# Scale (LSTM benefits from scaling). Fit on TRAIN INPUT rows only; apply to all.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.loc[:raw_end_for_train_inputs, feat_cols].values)\n",
    "df[feat_cols] = scaler.transform(df[feat_cols].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347c3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- BUILD SEQUENCES AFTER CLEANING/SCALING -------------------\n",
    "X_seq, y_seq, limit = build_sequences_seq(df, feat_cols, TARGET_COL, WINDOW_SIZE, PREDICT_HORIZON)\n",
    "N, T, F = X_seq.shape\n",
    "H = y_seq.shape[1]\n",
    "assert T == WINDOW_SIZE and H == PREDICT_HORIZON\n",
    "\n",
    "# ------------------- TRAIN/VAL SPLIT (chronological 80/20) -------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9dc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- LSTM MODEL -------------------\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, horizon):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, horizon)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):               # x: (B, T, F)\n",
    "        out, _ = self.lstm(x)           # (B, T, HIDDEN)\n",
    "        last = out[:, -1, :]            # last time step\n",
    "        return self.head(last)          # (B, horizon)\n",
    "\n",
    "model = LSTMForecast(F, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, H).to(device)\n",
    "criterion = nn.SmoothL1Loss()   # Huber\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "\n",
    "# DataLoaders\n",
    "train_dl = DataLoader(TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    "), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "val_dl = DataLoader(TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.float32)\n",
    "), batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f0787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Val MAE: 76.3324\n",
      "Epoch 02 | Val MAE: 75.1567\n",
      "Epoch 03 | Val MAE: 70.4450\n",
      "Epoch 04 | Val MAE: 62.2746\n",
      "Epoch 05 | Val MAE: 51.3796\n",
      "Epoch 06 | Val MAE: 37.4538\n",
      "Epoch 07 | Val MAE: 20.6191\n",
      "Epoch 08 | Val MAE: 5.3782\n",
      "Epoch 09 | Val MAE: 11.8420\n",
      "Epoch 10 | Val MAE: 11.0473\n",
      "Epoch 11 | Val MAE: 5.6196\n",
      "Epoch 12 | Val MAE: 4.4388\n",
      "Epoch 13 | Val MAE: 4.5521\n",
      "Epoch 14 | Val MAE: 5.5322\n",
      "Epoch 15 | Val MAE: 6.4659\n",
      "Epoch 16 | Val MAE: 6.2944\n",
      "Epoch 17 | Val MAE: 6.0180\n",
      "Epoch 18 | Val MAE: 5.7940\n",
      "Epoch 19 | Val MAE: 5.6868\n",
      "Epoch 20 | Val MAE: 5.6766\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------- TRAIN -------------------\n",
    "best_val_mae = float(\"inf\")\n",
    "best_state = None\n",
    "wait = 0\n",
    "\n",
    "def eval_loader(dataloader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            pred = model(xb)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "            trues.append(yb.cpu().numpy())\n",
    "    return np.vstack(preds), np.vstack(trues)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(device); yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate\n",
    "    val_pred, val_true = eval_loader(val_dl)\n",
    "    val_mae  = mean_absolute_error(val_true, val_pred)\n",
    "    scheduler.step(val_mae)\n",
    "    print(f\"Epoch {ep:02d} | Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "    if val_mae < best_val_mae - 1e-4:\n",
    "        best_val_mae = val_mae\n",
    "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc4cc727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train Metrics (averaged over 72 horizons) ===\n",
      "MAE:  10.937\n",
      "RMSE: 13.375\n",
      "R²:   -0.001\n",
      "First 12 horizons MAE: [11.205, 11.201, 11.195, 11.187, 11.176, 11.167, 11.161, 11.149, 11.14, 11.14, 11.123, 11.115]\n",
      "24h MAE: 11.046\n",
      "48h MAE: 10.841\n",
      "72h MAE: 10.667\n",
      "\n",
      "=== Validation Metrics (averaged over 72 horizons) ===\n",
      "MAE:  5.677\n",
      "RMSE: 7.230\n",
      "R²:   -0.719\n",
      "First 12 horizons MAE: [4.489, 4.529, 4.567, 4.599, 4.613, 4.648, 4.7, 4.717, 4.753, 4.849, 4.841, 4.879]\n",
      "24h MAE: 5.361\n",
      "48h MAE: 5.982\n",
      "72h MAE: 6.737\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------- FINAL METRICS (Train + Val) -------------------\n",
    "train_pred, train_true = eval_loader(train_dl)\n",
    "val_pred,   val_true   = eval_loader(val_dl)\n",
    "\n",
    "def metrics_block(y_t, y_p, label):\n",
    "    mae  = mean_absolute_error(y_t, y_p)\n",
    "    rmse = mean_squared_error(y_t, y_p, squared=False)\n",
    "    # Per-horizon R² then average (like your LGBM approach)\n",
    "    r2s = [r2_score(y_t[:, h], y_p[:, h]) for h in range(H)]\n",
    "    print(f\"\\n=== {label} Metrics (averaged over {H} horizons) ===\")\n",
    "    print(f\"MAE:  {mae:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"R²:   {np.mean(r2s):.3f}\")\n",
    "    # First 12 + specific horizons\n",
    "    mae_list  = [mean_absolute_error(y_t[:, h], y_p[:, h]) for h in range(H)]\n",
    "    if H >= 12: print(f\"First 12 horizons MAE: {[round(m,3) for m in mae_list[:12]]}\")\n",
    "    if H >= 24: print(f\"24h MAE: {mae_list[23]:.3f}\")\n",
    "    if H >= 48: print(f\"48h MAE: {mae_list[47]:.3f}\")\n",
    "    if H >= 72: print(f\"72h MAE: {mae_list[71]:.3f}\")\n",
    "    return mae_list, r2s\n",
    "\n",
    "train_mae_list, train_r2_list = metrics_block(train_true, train_pred, \"Train\")\n",
    "val_mae_list,   val_r2_list   = metrics_block(val_true,   val_pred,   \"Validation\")\n",
    "\n",
    "# Save per-horizon validation metrics\n",
    "val_report = pd.DataFrame({\n",
    "    \"horizon\": np.arange(1, H+1),\n",
    "    \"MAE\": val_mae_list,\n",
    "    \"R2\":  val_r2_list\n",
    "})\n",
    "val_report.to_csv(OUT_DIR / \"lstm_val_per_horizon_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "262c9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved forecast → d:\\Desktop\\AlinasPrograms\\myenv\\10Pearls2\\predictions\\lstm_predicted_aqi_72hrs.csv\n",
      "First/last timestamps: 10/8/25 00:00 → 12/8/25 23:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------- FORECAST NEXT 72 HOURS (ONE CSV) -------------------\n",
    "# last window of *scaled* features (after cleaning)\n",
    "last_window = df[feat_cols].values[-WINDOW_SIZE:]              # (T, F)\n",
    "last_window_t = torch.tensor(last_window, dtype=torch.float32).unsqueeze(0).to(device)  # (1, T, F)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    future_pred = model(last_window_t).cpu().numpy().reshape(-1)   # (72,)\n",
    "\n",
    "# anchor = last non-NaT time; start = +1 hour\n",
    "last_valid_time = df.loc[df[TIME_COL].notna(), TIME_COL].iloc[-1]\n",
    "start = last_valid_time.floor(\"H\") + pd.Timedelta(hours=1)\n",
    "future_times = pd.date_range(start=start, periods=PREDICT_HORIZON, freq=\"h\")\n",
    "\n",
    "# format as d/m/yy HH:MM (so 9/8/25 = 9 Aug 2025)\n",
    "ft = pd.Series(future_times)\n",
    "formatted_dt = (\n",
    "    ft.dt.day.astype(str) + \"/\" +\n",
    "    ft.dt.month.astype(str) + \"/\" +\n",
    "    ft.dt.strftime(\"%y\") + \" \" +\n",
    "    ft.dt.strftime(\"%H:%M\")\n",
    ")\n",
    "\n",
    "forecast_df = pd.DataFrame({\n",
    "    \"datetime\": formatted_dt,\n",
    "    \"predicted_aqi_us\": future_pred\n",
    "})\n",
    "forecast_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nSaved forecast → {OUTPUT_FILE}\")\n",
    "print(\"First/last timestamps:\", forecast_df['datetime'].iloc[0], \"→\", forecast_df['datetime'].iloc[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77c30e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model → d:\\Desktop\\AlinasPrograms\\myenv\\10Pearls2\\models\\current\\lstm_multioutput_72h.pt\n",
      "Saved scaler → d:\\Desktop\\AlinasPrograms\\myenv\\10Pearls2\\models\\current\\scaler.joblib\n",
      "Saved metadata → d:\\Desktop\\AlinasPrograms\\myenv\\10Pearls2\\models\\current\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------- SAVE MODEL + METADATA + SCALER -------------------\n",
    "SAVE_DIR = PROJECT_ROOT / \"models\" / \"current\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH   = SAVE_DIR / \"lstm_multioutput_72h.pt\"\n",
    "SCALER_PATH  = SAVE_DIR / \"scaler.joblib\"\n",
    "META_PATH    = SAVE_DIR / \"metadata.json\"\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "\n",
    "meta = {\n",
    "    \"features\": feat_cols,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"time_col\": TIME_COL,\n",
    "    \"window_size\": WINDOW_SIZE,\n",
    "    \"horizon\": PREDICT_HORIZON,\n",
    "    \"dayfirst\": True,\n",
    "    \"winsor_low\": low,\n",
    "    \"winsor_high\": high,\n",
    "    \"train_medians\": train_medians,\n",
    "    \"scaler_path\": str(SCALER_PATH)\n",
    "}\n",
    "META_PATH.write_text(json.dumps(meta, indent=2))\n",
    "print(\"Saved model →\", MODEL_PATH)\n",
    "print(\"Saved scaler →\", SCALER_PATH)\n",
    "print(\"Saved metadata →\", META_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
